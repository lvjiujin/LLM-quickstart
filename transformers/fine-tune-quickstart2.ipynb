{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c6730f-5d76-450b-9788-ec883d024f57",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers 微调训练入门\n",
    "\n",
    "本示例将介绍基于 Transformers 实现模型微调训练的主要流程，包括：\n",
    "- 数据集下载\n",
    "- 数据预处理\n",
    "- 训练超参数配置\n",
    "- 训练评估指标设置\n",
    "- 训练器基本介绍\n",
    "- 实战训练\n",
    "- 模型保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b1e12-1921-4438-8d5d-9760a629dcfe",
   "metadata": {},
   "source": [
    "## YelpReviewFull 数据集\n",
    "\n",
    "**Hugging Face 数据集：[ YelpReviewFull ](https://huggingface.co/datasets/yelp_review_full)**\n",
    "\n",
    "### 数据集摘要\n",
    "\n",
    "Yelp评论数据集包括来自Yelp的评论。它是从Yelp Dataset Challenge 2015数据中提取的。\n",
    "\n",
    "### 支持的任务和排行榜\n",
    "文本分类、情感分类：该数据集主要用于文本分类：给定文本，预测情感。\n",
    "\n",
    "### 语言\n",
    "这些评论主要以英语编写。\n",
    "\n",
    "### 数据集结构\n",
    "\n",
    "#### 数据实例\n",
    "一个典型的数据点包括文本和相应的标签。\n",
    "\n",
    "来自YelpReviewFull测试集的示例如下：\n",
    "\n",
    "```json\n",
    "{\n",
    "    'label': 0,\n",
    "    'text': 'I got \\'new\\' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\\\nI took the tire over to Flynn\\'s and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he\\'d give me a new tire \\\\\"this time\\\\\". \\\\nI will never go back to Flynn\\'s b/c of the way this guy treated me and the simple fact that they gave me a used tire!'\n",
    "}\n",
    "```\n",
    "\n",
    "#### 数据字段\n",
    "\n",
    "- 'text': 评论文本使用双引号（\"）转义，任何内部双引号都通过2个双引号（\"\"）转义。换行符使用反斜杠后跟一个 \"n\" 字符转义，即 \"\\n\"。\n",
    "- 'label': 对应于评论的分数（介于1和5之间）。\n",
    "\n",
    "#### 数据拆分\n",
    "\n",
    "Yelp评论完整星级数据集是通过随机选取每个1到5星评论的130,000个训练样本和10,000个测试样本构建的。总共有650,000个训练样本和50,000个测试样本。\n",
    "\n",
    "## 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf72d6c-7ea5-4ee1-969a-c5060b9cb2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6fc806-1395-42dd-8121-a6e98a95cf01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c94ad529-1604-48bd-8c8d-aa2f3bca6200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 2,\n",
       " 'text': \"As far as Starbucks go, this is a pretty nice one.  The baristas are friendly and while I was here, a lot of regulars must have come in, because they bantered away with almost everyone.  The bathroom was clean and well maintained and the trash wasn't overflowing in the canisters around the store.  The pastries looked fresh, but I didn't partake.  The noise level was also at a nice working level - not too loud, music just barely audible.\\\\n\\\\nI do wish there was more seating.  It is nice that this location has a counter at the end of the bar for sole workers, but it doesn't replace more tables.  I'm sure this isn't as much of a problem in the summer when there's the space outside.\\\\n\\\\nThere was a treat receipt promo going on, but the barista didn't tell me about it, which I found odd.  Usually when they have promos like that going on, they ask everyone if they want their receipt to come back later in the day to claim whatever the offer is.  Today it was one of their new pastries for $1, I know in the summer they do $2 grande iced drinks with that morning's receipt.\\\\n\\\\nOverall, nice working or socializing environment.  Very friendly and inviting.  It's what I've come to expect from Starbucks, so points for consistency.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc45997-e391-456f-b0b9-d3193b0f6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e2ecebb-d5d1-456d-967c-842a79fdd622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1af560b6-7d21-499e-9b82-114be371a98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 star</td>\n",
       "      <td>For standard breakfast food this is just ok.  The problem is there is not a whole lot around the Nugget to get breakfast.  If you stick to the basics it will be average.  The service for us the morning we ate here was very slow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 star</td>\n",
       "      <td>I've been to Prime several times, with different friends, for different occasions.  We don't stay at Bellagio, but always made the trip to Prime.  Had I been yelping when we first started going there, this would be a 5 star review.  2 times ago, 3 of us went, for my birthday, and noticed the decline in the food.  Still,the service, and having desert and coffee on the Terrace afterwards, with a front row view of the fabulous fountain show more than made up for it.  The last time (2011) when I called to make reservations, I stated that we would like coffee on the Terrace, after dinner.  I was told that you could now have your entire meal on the Terrace!  That was great news, except that you could not reserve a table there, but could take your chances.  Why??  I was calling 2 weeks ahead.  Ultimately, we were seated outdoors, as requested.  Our waiter, Luis, I think,  was RUDE and he and the mini waiters helping him, were slow and inefficient.  Our meals finally came, and my daughter's steak was wrong.  She ordered a medium rare, Kobe filet mignon, and was brought  a dried out, well done, New York.  The waiter was upset, and insisted that was what she had ordered.  The New York had never even been considered, and we told this idiot, when we placed our order, that we were anxious to compare the regular filet that I ordered, to the Kobe filet.  He said the kitchen would be \\\"mad\\\" if we sent it back.  We sent it back anyway. Pretty brave on our parts!  It took 1/2 hour to bring what she had actually ordered.  In the meantime, my meal, and our sides, were getting cold, so we ate from my plate.  Our bill was over $400, for 2 people.  I didn't want to raise hell, and spoil what was left of my daughter's birthday dinner, but did email the manger when I got home.  I gave her a detailed description of our experience.  As the manager, she should have appreciated  the polite, constructive and honest feedback.  Her email back said that she was sorry, and was looking into the matter.  She said she did not want to lose us as future guests to Prime, and would I please send my phone number so she could call and discuss the matter further.  I sent my phone number, and never heard from her, again.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>where I am from dogs are not allowed at farmer's markets due to health code dealio reasons... so it was a shock to see so many dogs around (Scottsdale seems to be super pet friendly). \\n\\nI had a blast sampling all the goodies... lots of great things to choose from!! There is no need to eat prior to the market-- between the samples, the food and the beverages not eating was the best thing ever. \\n\\nAnother thing I appreciated.. this market was totally calm, relaxed... most farmer's markets back home (minus Santa Cruz) is super uptight and folks are pushy..... definitely a spot to stroll, eat and enjoy the morning...... \\n\\nThe Farm Sessions concept I LOVED.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>So I stayed at one of my girl's room and I love the decor. I have always enjoyed such beautiful decor. All the details in this hotel is very elegant. The room itself is pretty nice. I love the open spaced bathroom, but the toilet room's door has no lock! LoL very strange... Overall, very nice! =D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 star</td>\n",
       "      <td>We only came here because La Palapa was full with a 45 minute wait, we were starving, this was across the street, and they could seat us immediately. Sad disappointment. Next time I'll get takeout from La Palapa, or make a reservation there. \\n\\nTo be fair, the chipotle salsa was okay, but the chips were extra tough, and the pile was about four times too large for the amount of salsa.\\n\\nWe tried the beet tacos (interesting, but not amazing), and the fish tacos (downright bad). The rice and beans included as side dishes weren't great either. I think this place mainly caters to the college drinking crowd, but I wouldn't go here for the food. The bathroom was also completely out of soap which is a little gross.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>I went to see Dr. Underwood as I needed a new PCP.  The office has very limited hours to see a doc so I had to wait 2 months, which since there was nothing wrong, no problem, I was just going in for a checkup.\\n\\nI filled out the paperwork beforehand and faxed it over trying to cut down on time.  This would have been fine except the paperwork was not put in the system for us when we got there, so no time saved.\\n\\nThe office staff is friendly albeit slow.  The PA we saw was FANTASTIC.  She spent an hour with me, going over medical history, information and answered all of my questions and didn't hurry at all.  I was FLOORED. I haven't been to a doctor who has given me that much time, well, ever.\\n\\nThis is the reason I will continue to go back and be satisfied.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>Yes I was here, the fountains are very pretty from the street (they are better from the Eiffel tower Paris across the street) I do love the fountains a must see when in Vegas..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>12/03/09\\nDecided I'd go to CB to find out once and for all what roasted veal marrow tasted like after hearing a couple of Food Network celebrity chefs hark on and on about how wonderful they are (Anne Burrell, in particular, spoke of it on an episode of \\\"The Best Thing I Ever Ate\\\").  I'd have to say that I didn't think much of them.  The presentation was beautiful, I must admit, but paying $10 for what amounted to about a teaspoon and a half (OK, for argument's sake, let's round it off to two teaspoons) of frothy fat accented with Hawai'ian sea salt and tawny port reduction did not translate to a great experience for me (taste, nutrition, or cost).  Sure, there were a bit of bitter greens and three toasted pieces of bread (which two teaspoons of fat could not even cover), but I found more joy and sustenance w/ the singular toast slice slathered w/ goat cheese (forgive me, it's been awhile since I had this app, but the spread could've been mascarpone and ricotta, but I want to say goat cheese) and slices of Campari tomatoes.  Happy hour price for this app was $4.50, which I found easier to swallow, pardon the pun, than to pay $9 for a singular slice of toast w/ a soft-cheese spread and a little bitter greens on top.  (I can't eat tomatoes, so those gourmet slices just ended up in the garbage bin.)  It was very tasty, however, so I may order it again, but I won't be very happy about it.  Perhaps I could get a substitute for the tomatoes?  I will ask next time I go.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>This might be my favorite Starbucks on any of my usual routes. The parking lot and surrounding road construction is a little on the annoying side, but if you plan for it there's usually no problem.\\n\\nThe inside is on the smaller side. Not a lot of indoor seating but a good amount of tables outside. I've never been here when there are less than 3 people working behind the counter, and I'm quite impressed that they are as efficient as they are with such limited space.\\n\\nMy drink here has always been correct, quick, and service is always friendly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2 star</td>\n",
       "      <td>This place always seems perpetually understaffed. Everyone who works there is courteous, but there's the potential for a 15-minute wait no matter what time of day you go. Do not go here if you're in a rush, unless you're just dropping off letters. Otherwise you're likely to get stuck behind a blue-hair who doesn't understand the concept of \\\"properly filling out your forms before you're at the counter.\\\" The stamp kiosk is useful, though, especially if you're into buying stamps at weird hours -- the front lobby is open 24/7!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df7cd0-23cd-458f-b2b5-f025c3b9fe62",
   "metadata": {},
   "source": [
    "## 预处理数据\n",
    "\n",
    "下载数据集到本地后，使用 Tokenizer 来处理文本，对于长度不等的输入数据，可以使用填充（padding）和截断（truncation）策略来处理。\n",
    "\n",
    "Datasets 的 `map` 方法，支持一次性在整个数据集上应用预处理函数。\n",
    "\n",
    "下面使用填充到最大长度的策略，处理整个数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bf2b342-e1dd-4ab6-ad57-28eb2513ae38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vipuser/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47a415a8-cd15-4a8c-851b-9b4740ef8271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>I must say, I am impressed by Yogli Mogli!\\n\\nThe interior is bright and cheerful, towards the back there is a copious amount of tables and chairs, and everything is clean.\\n\\nMost of the yogurt flavors were sweet, they only had 2 tart varieties - plain and raspberry. There are plenty of candy and cereal toppings as well as fresh fruits and popping pearls. \\n\\nThere are 2 tables to sit at that are outside the store front. bonus points to the site for pitting out a water bowl for the dogs passing by with their owners.</td>\n",
       "      <td>[101, 146, 1538, 1474, 117, 146, 1821, 7351, 1118, 14941, 23655, 12556, 23655, 106, 165, 183, 165, 183, 1942, 4638, 4604, 1110, 3999, 1105, 20710, 117, 2019, 1103, 1171, 1175, 1110, 170, 9947, 4179, 2971, 1104, 7072, 1105, 8391, 117, 1105, 1917, 1110, 4044, 119, 165, 183, 165, 183, 2107, 15540, 1104, 1103, 26063, 13830, 3740, 16852, 1116, 1127, 4105, 117, 1152, 1178, 1125, 123, 27629, 3740, 10003, 118, 6188, 1105, 187, 2225, 1643, 6614, 119, 1247, 1132, 7722, 1104, 13962, 1105, 24759, 27001, 1116, 1112, 1218, 1112, 4489, 11669, 1105, 22850, 24837, 1116, 119, 165, 183, 165, 183, 1942, ...]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(tokenized_datasets[\"train\"], num_examples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c33d153-f729-4f04-972c-a764c1cbbb8b",
   "metadata": {},
   "source": [
    "### 数据抽样\n",
    "\n",
    "使用 1000 个数据样本，在 BERT 上演示小规模训练（基于 Pytorch Trainer）\n",
    "\n",
    "`shuffle()`函数会随机重新排列列的值。如果您希望对用于洗牌数据集的算法有更多控制，可以在此函数中指定generator参数来使用不同的numpy.random.Generator。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a17317d8-3c6a-467f-843d-87491f600db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b65d63-2d3a-4a56-bc31-6e88a29e9dec",
   "metadata": {},
   "source": [
    "## 微调训练配置\n",
    "\n",
    "### 加载 BERT 模型\n",
    "\n",
    "警告通知我们正在丢弃一些权重（`vocab_transform` 和 `vocab_layer_norm` 层），并随机初始化其他一些权重（`pre_classifier` 和 `classifier` 层）。在微调模型情况下是绝对正常的，因为我们正在删除用于预训练模型的掩码语言建模任务的头部，并用一个新的头部替换它，对于这个新头部，我们没有预训练的权重，所以库会警告我们在用它进行推理之前应该对这个模型进行微调，而这正是我们要做的事情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d2af4df-abd4-4a4b-94b6-b0e7375304ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# 因为出现了trainer.train()中断的情况，所以我这里采用断点续训的方式。\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n",
    "# /root/LLM-quickstart/transformers/models/bert-base-cased-finetune-yelp/checkpoint-37000\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/data/LLM-quickstart/transformers/models/bert-base-cased-finetune-yelp/checkpoint-56500\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44014df-b52c-4c72-9e9f-54424725a473",
   "metadata": {},
   "source": [
    "### 训练超参数（TrainingArguments）\n",
    "\n",
    "完整配置参数与默认值：https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "\n",
    "源代码定义：https://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/training_args.py#L161\n",
    "\n",
    "**最重要配置：模型权重保存路径(output_dir)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98c01d5c-de72-4ff0-b11d-e07ac5346888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "model_dir = \"/data2/models/bert-base-cased-finetune-yelp\"\n",
    "\n",
    "# logging_steps 默认值为500，根据我们的训练数据和步长，将其设置为1000\n",
    "training_args = TrainingArguments(output_dir=model_dir,\n",
    "                                  per_device_train_batch_size=32,\n",
    "                                  # auto_find_batch_size=True,\n",
    "                                  num_train_epochs=3,\n",
    "                                  logging_steps=1000,\n",
    "                                 save_steps=1000,\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ce03480-3aaa-48ea-a0c6-a177b8d8e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/data2/models/bert-base-cased-finetune-yelp/runs/Apr03_21-48-30_ubuntu22,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1000,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/data2/models/bert-base-cased-finetune-yelp,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/data2/models/bert-base-cased-finetune-yelp,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=1000,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 完整的超参数配置\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebd3365-d359-4ab4-a300-4717590cc240",
   "metadata": {},
   "source": [
    "### 训练过程中的指标评估（Evaluate)\n",
    "\n",
    "**[Hugging Face Evaluate 库](https://huggingface.co/docs/evaluate/index)** 支持使用一行代码，获得数十种不同领域（自然语言处理、计算机视觉、强化学习等）的评估方法。 当前支持 **完整评估指标：https://huggingface.co/evaluate-metric**\n",
    "\n",
    "训练器（Trainer）在训练过程中不会自动评估模型性能。因此，我们需要向训练器传递一个函数来计算和报告指标。 \n",
    "\n",
    "Evaluate库提供了一个简单的准确率函数，您可以使用`evaluate.load`函数加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a8ef138-5bf2-41e5-8c68-df8e11f4e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d406c0-56d0-4a54-9c6c-e126ab7f5254",
   "metadata": {},
   "source": [
    "\n",
    "接着，调用 `compute` 函数来计算预测的准确率。\n",
    "\n",
    "在将预测传递给 compute 函数之前，我们需要将 logits 转换为预测值（**所有Transformers 模型都返回 logits**）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f46d2e59-1ebf-43d2-bc86-6b57a4d24d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2feba67-9ca9-4793-9a15-3eaa426df2a1",
   "metadata": {},
   "source": [
    "#### 训练过程指标监控\n",
    "\n",
    "通常，为了监控训练过程中的评估指标变化，我们可以在`TrainingArguments`指定`evaluation_strategy`参数，以便在 epoch 结束时报告评估指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afaaee18-4986-4e39-8ad9-b8d413ab4cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=model_dir,\n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  per_device_train_batch_size=32,\n",
    "                                  # auto_find_batch_size=True,\n",
    "                                  num_train_epochs=3,\n",
    "                                  logging_steps=1000,\n",
    "                                  save_steps=1000,\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d6981-e444-4c0f-a7cb-dd7f2ba8df12",
   "metadata": {},
   "source": [
    "## 开始训练\n",
    "\n",
    "### 实例化训练器（Trainer）\n",
    "\n",
    "`kernel version` 版本问题：暂不影响本示例代码运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca1d12ac-89dc-4c30-8282-f859724c0062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vipuser/miniconda3/envs/llm/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833e0db-1168-4a3c-8b75-bfdcef8c5157",
   "metadata": {},
   "source": [
    "## 使用 nvidia-smi 查看 GPU 使用\n",
    "\n",
    "为了实时查看GPU使用情况，可以使用 `watch` 指令实现轮询：`watch -n 1 nvidia-smi`:\n",
    "\n",
    "batch_size = 32\n",
    "```shell\n",
    "Every 1.0s: nvidia-smi                                                                                                                                           ubuntu22: Thu Apr  4 21:07:33 2024\r\n",
    "\r\n",
    "Thu Apr  4 21:07:33 2024\r\n",
    "+---------------------------------------------------------------------------------------+\r\n",
    "| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |\r\n",
    "|-----------------------------------------+----------------------+----------------------+\r\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
    "|                                         |                      |               MIG M. |\r\n",
    "|=========================================+======================+======================|\r\n",
    "|   0  Tesla V100S-PCIE-32GB          Off | 00000000:00:08.0 Off |                    0 |\r\n",
    "| N/A   58C    P0             235W / 250W |  22048MiB / 32768MiB |    100%      Default |\r\n",
    "|                                         |                      |                  N/A |\r\n",
    "+-----------------------------------------+----------------------+----------------------+\r\n",
    "\r\n",
    "+---------------------------------------------------------------------------------------+\r\n",
    "| Processes:                                                                            |\r\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
    "|        ID   ID                                                             Usage      |\r\n",
    "|=======================================================================================|\r\n",
    "|    0   N/A  N/A     23543      C   .../miniconda3/envs/llm/bin/python3.11    22044MiB |\r\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11c43bc-db4e-43c0-a780-dfe36c487b65",
   "metadata": {},
   "source": [
    "## 当训练的时候，出现IOPub message rate exceeded时"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd766350-fd78-4abc-a0dc-6630b815470b",
   "metadata": {},
   "source": [
    "\n",
    "![IOPub message rate exceeded](images/IOPub_message_rate_exceeded.png)\n",
    "\n",
    "此时，后台的程序实际上一直没有停止，而且还在继续训练，只是没有往客户端发送数据而已。\n",
    "我们可以设置参数：ServerApp.iopub_msg_rate_limit=10000000000.0 。\n",
    "有两种设置方法：\n",
    "\n",
    "一种是在启动jupyter lab的命令行中进行设置,即：\n",
    "\n",
    "`nohup /home/vipuser/anaconda3/envs/llm/bin/python3.11  /home/vipuser/anaconda3/envs/llm/bin/jupyter-lab --no-browser --ip 0.0.0.0  --port 3389 --config=/root/jupyter/jupyter_server_config.json --allow-root --notebook-dir=/root/LLM-quickstart --ServerApp.iopub_msg_rate_limit=10000000000.0 & `\n",
    "\n",
    "\n",
    "一种是直接用 `jupyter lab --generate-config` 命令生成jupyter_server_config.py文件，在文件中进行修改。\n",
    "\n",
    "值得注意的是，当我们提到说要对jupyter lab进行设置的时候，实际上需要杀掉jupyter lab的进程的。所以进行这一步之前，我们需要先做好断点续训的保存工作。往下看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "accfe921-471d-481a-96da-c491cdebad0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60939' max='60939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60939/60939 15:59:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.608000</td>\n",
       "      <td>0.775245</td>\n",
       "      <td>0.681920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.520600</td>\n",
       "      <td>0.827644</td>\n",
       "      <td>0.679800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.351500</td>\n",
       "      <td>0.978556</td>\n",
       "      <td>0.672200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60939, training_loss=0.4875707762025281, metrics={'train_runtime': 57573.0971, 'train_samples_per_second': 33.87, 'train_steps_per_second': 1.058, 'total_flos': 5.130803778048e+17, 'train_loss': 0.4875707762025281, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f6308e-c7d3-4bd6-a1de-4da9526a99be",
   "metadata": {},
   "source": [
    "## 分析一下，为何loss 越来越小，而在验证集上效果却越来越差。\n",
    "\n",
    "\n",
    "可能是过拟合，实际上因为中断以及断点处继续训练，我训练的epoch接近了6个。所以看到目前的第一个epoch实际上可能是第四个，后面第2个epoch可能是第5个，以此类推。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175cc346-223e-4500-8313-fe5a748fb6ab",
   "metadata": {},
   "source": [
    "## 断点续训练，当trainer.train()中断后，继续训练\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/root/LLM-quickstart/transformers/models/bert-base-cased-finetune-yelp/checkpoint-37000\", num_labels=5)\n",
    "\n",
    "从断点处保存的模型开始训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d581099-37a4-4470-b051-1ada38554089",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffb47eab-1370-491e-8a84-6d5347a350b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 08:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9785563349723816,\n",
       " 'eval_accuracy': 0.6722,\n",
       " 'eval_runtime': 533.1961,\n",
       " 'eval_samples_per_second': 93.774,\n",
       " 'eval_steps_per_second': 11.722,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a55686-7c43-4ab8-a5cd-0e77f14c7c52",
   "metadata": {},
   "source": [
    "### 保存模型和训练状态\n",
    "\n",
    "- 使用 `trainer.save_model` 方法保存模型，后续可以通过 from_pretrained() 方法重新加载\n",
    "- 使用 `trainer.save_state` 方法保存训练状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad0cbc14-9ef7-450f-a1a3-4f92b6486f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "badf5868-2847-439d-a73e-42d1cca67b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd92e35d-fed7-4ff2-aa84-27b5e29b917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.model.save_pretrained(\"./\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
